import streamlit as st
import google.generativeai as genai
from datetime import datetime
import time
import os

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="Chatbot Gemini",
    page_icon="ü§ñ",
    layout="centered",
    initial_sidebar_state="expanded"
)

# T√≠tulo e descri√ß√£o
st.title("üí¨ Chatbot com Google Gemini")
st.markdown("Converse com um chatbot alimentado pela intelig√™ncia artificial do Google Gemini.")

# Inicializa√ß√£o do estado da sess√£o
if "messages" not in st.session_state:
    st.session_state.messages = []
if "api_key" not in st.session_state:
    st.session_state.api_key = ""
if "model" not in st.session_state:
    st.session_state.model = "gemini-1.5-flash"

# Sidebar para configura√ß√µes
with st.sidebar:
    st.header("‚öôÔ∏è Configura√ß√µes")
    
    # Input para a API Key
    api_key = st.text_input(
        "Chave da API Gemini", 
        type="password",
        value=st.session_state.api_key,
        help="Obtenha sua chave em: https://aistudio.google.com/"
    )
    
    # Sele√ß√£o do modelo
    model_option = st.selectbox(
        "Modelo Gemini",
        ["gemini-1.5-flash", "gemini-1.0-pro"],
        index=0 if st.session_state.model == "gemini-1.5-flash" else 1
    )
    
    # Configura√ß√µes de gera√ß√£o
    temperature = st.slider("Temperatura", 0.0, 1.0, 0.7, help="Controla a criatividade das respostas. Valores mais altos = mais criativo.")
    max_tokens = st.slider("Tokens m√°ximos", 1, 2048, 1024, help="Limite m√°ximo de tokens por resposta.")
    
    # Bot√£o para limpar hist√≥rico
    if st.button("üßπ Limpar Conversa"):
        st.session_state.messages = []
        st.rerun()
    
    st.divider()
    st.markdown("### üìä Sobre")
    st.markdown("""
    Este chatbot utiliza a API do Google Gemini para gerar respostas.
    - Desenvolvido com Streamlit
    - Modelos: Gemini 1.5 Flash ou Gemini 1.0 Pro
    - C√≥digo aberto no GitHub
    """)

# Atualizar configura√ß√µes na sess√£o
st.session_state.api_key = api_key
st.session_state.model = model_option

# Verificar se a API key foi fornecida
if not st.session_state.api_key:
    st.info("üëà Por favor, adicione sua API Key do Gemini na barra lateral para come√ßar a conversar.")
    st.stop()

# Configurar a API do Gemini
try:
    genai.configure(api_key=st.session_state.api_key)
    
    # Configura√ß√£o do modelo
    generation_config = {
        "temperature": temperature,
        "top_p": 1,
        "top_k": 1,
        "max_output_tokens": max_tokens,
    }
    
    # Inicializar o modelo
    model = genai.GenerativeModel(
        model_name=st.session_state.model,
        generation_config=generation_config
    )
except Exception as e:
    st.error(f"Erro ao configurar a API: {str(e)}")
    st.stop()

# Exibir mensagens do chat
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "timestamp" in message:
            st.caption(f"{message['timestamp']}")

# Input do usu√°rio
if prompt := st.chat_input("Digite sua mensagem..."):
    # Adicionar mensagem do usu√°rio ao hist√≥rico
    timestamp = datetime.now().strftime("%H:%M:%S")
    st.session_state.messages.append({"role": "user", "content": prompt, "timestamp": timestamp})
    
    # Exibir mensagem do usu√°rio
    with st.chat_message("user"):
        st.markdown(prompt)
        st.caption(timestamp)
    
    # Gerar resposta do assistente
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        message_placeholder.markdown("‚ñå")
        
        try:
            # Construir hist√≥rico de conversa para o modelo
            history_for_model = []
            for msg in st.session_state.messages[:-1]:  # Excluir a √∫ltima mensagem (a atual)
                role = "user" if msg["role"] == "user" else "model"
                history_for_model.append({role: msg["content"]})
            
            # Gerar resposta
            response = model.generate_content(
                contents=history_for_model + [{"user": prompt}]
            )
            
            # Exibir resposta
            full_response = response.text
            message_placeholder.markdown(full_response)
            
            # Adicionar resposta ao hist√≥rico
            timestamp = datetime.now().strftime("%H:%M:%S")
            st.session_state.messages.append({"role": "assistant", "content": full_response, "timestamp": timestamp})
            
            # Exibir timestamp
            st.caption(timestamp)
            
        except Exception as e:
            error_message = f"Desculpe, ocorreu um erro: {str(e)}"
            message_placeholder.markdown(error_message)
            timestamp = datetime.now().strftime("%H:%M:%S")
            st.session_state.messages.append({"role": "assistant", "content": error_message, "timestamp": timestamp})
            st.caption(timestamp)
